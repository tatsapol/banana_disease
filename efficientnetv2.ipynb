{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "original+augment"
      ],
      "metadata": {
        "id": "BHEHVykp5dS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define the directory where your dataset is stored on Google Drive\n",
        "data_dir = \"/content/drive/MyDrive/original+augment\"  # Replace with your dataset's path\n",
        "batch_size = 32\n",
        "num_classes = 6\n",
        "\n",
        "# Step 3: Define transformations for the dataset\n",
        "basic_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Step 4: Load the dataset from Google Drive\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=basic_transforms)\n",
        "\n",
        "# Step 5: Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Step 6: Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Step 7: Load the EfficientNet B2 model and modify its classifier\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.efficientnet_b2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 8: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 9: Define the training function\n",
        "def train(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "# Step 10: Define the validation function\n",
        "def validate(model, loader, criterion, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    tp = [0] * num_classes\n",
        "    fp = [0] * num_classes\n",
        "    fn = [0] * num_classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Calculate TP, FP, FN for each class\n",
        "            for i in range(num_classes):\n",
        "                tp[i] += torch.sum((preds == i) & (labels == i)).item()\n",
        "                fp[i] += torch.sum((preds == i) & (labels != i)).item()\n",
        "                fn[i] += torch.sum((preds != i) & (labels == i)).item()\n",
        "\n",
        "    accuracy = correct.double() / len(loader.dataset)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        print(f\"Class {i} - TP: {tp[i]}, FP: {fp[i]}, FN: {fn[i]}\")\n",
        "\n",
        "    return avg_loss, accuracy, tp, fp, fn\n",
        "\n",
        "# Step 11: Training loop\n",
        "num_epochs = 7\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy, tp, fp, fn = validate(model, val_loader, criterion, num_classes)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Step 12: Save the trained model to Google Drive\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/banana_disease_efficientnet_b2.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG4BK7ivt1Tj",
        "outputId": "bccfbdcd-b091-4557-f7ee-0b5ea23f77e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
            "100%|██████████| 35.2M/35.2M [00:00<00:00, 130MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 - TP: 84, FP: 1, FN: 10\n",
            "Class 1 - TP: 74, FP: 3, FN: 0\n",
            "Class 2 - TP: 70, FP: 7, FN: 0\n",
            "Class 3 - TP: 90, FP: 0, FN: 4\n",
            "Class 4 - TP: 84, FP: 4, FN: 3\n",
            "Class 5 - TP: 85, FP: 3, FN: 1\n",
            "Epoch 1/7\n",
            "Train Loss: 0.3547, Validation Loss: 0.1207, Validation Accuracy: 0.9644\n",
            "Class 0 - TP: 87, FP: 0, FN: 7\n",
            "Class 1 - TP: 70, FP: 0, FN: 4\n",
            "Class 2 - TP: 70, FP: 3, FN: 0\n",
            "Class 3 - TP: 93, FP: 3, FN: 1\n",
            "Class 4 - TP: 87, FP: 3, FN: 0\n",
            "Class 5 - TP: 86, FP: 3, FN: 0\n",
            "Epoch 2/7\n",
            "Train Loss: 0.1765, Validation Loss: 0.0707, Validation Accuracy: 0.9762\n",
            "Class 0 - TP: 94, FP: 0, FN: 0\n",
            "Class 1 - TP: 73, FP: 0, FN: 1\n",
            "Class 2 - TP: 70, FP: 7, FN: 0\n",
            "Class 3 - TP: 90, FP: 1, FN: 4\n",
            "Class 4 - TP: 87, FP: 4, FN: 0\n",
            "Class 5 - TP: 79, FP: 0, FN: 7\n",
            "Epoch 3/7\n",
            "Train Loss: 0.0849, Validation Loss: 0.0857, Validation Accuracy: 0.9762\n",
            "Class 0 - TP: 94, FP: 1, FN: 0\n",
            "Class 1 - TP: 74, FP: 0, FN: 0\n",
            "Class 2 - TP: 69, FP: 0, FN: 1\n",
            "Class 3 - TP: 94, FP: 0, FN: 0\n",
            "Class 4 - TP: 87, FP: 0, FN: 0\n",
            "Class 5 - TP: 86, FP: 0, FN: 0\n",
            "Epoch 4/7\n",
            "Train Loss: 0.0675, Validation Loss: 0.0132, Validation Accuracy: 0.9980\n",
            "Class 0 - TP: 88, FP: 1, FN: 6\n",
            "Class 1 - TP: 74, FP: 0, FN: 0\n",
            "Class 2 - TP: 70, FP: 11, FN: 0\n",
            "Class 3 - TP: 93, FP: 0, FN: 1\n",
            "Class 4 - TP: 87, FP: 1, FN: 0\n",
            "Class 5 - TP: 80, FP: 0, FN: 6\n",
            "Epoch 5/7\n",
            "Train Loss: 0.0457, Validation Loss: 0.0928, Validation Accuracy: 0.9743\n",
            "Class 0 - TP: 87, FP: 0, FN: 7\n",
            "Class 1 - TP: 71, FP: 0, FN: 3\n",
            "Class 2 - TP: 70, FP: 5, FN: 0\n",
            "Class 3 - TP: 94, FP: 0, FN: 0\n",
            "Class 4 - TP: 87, FP: 4, FN: 0\n",
            "Class 5 - TP: 85, FP: 2, FN: 1\n",
            "Epoch 6/7\n",
            "Train Loss: 0.0528, Validation Loss: 0.0627, Validation Accuracy: 0.9782\n",
            "Class 0 - TP: 86, FP: 0, FN: 8\n",
            "Class 1 - TP: 73, FP: 1, FN: 1\n",
            "Class 2 - TP: 70, FP: 3, FN: 0\n",
            "Class 3 - TP: 94, FP: 0, FN: 0\n",
            "Class 4 - TP: 87, FP: 5, FN: 0\n",
            "Class 5 - TP: 86, FP: 0, FN: 0\n",
            "Epoch 7/7\n",
            "Train Loss: 0.1132, Validation Loss: 0.0490, Validation Accuracy: 0.9822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "original\n"
      ],
      "metadata": {
        "id": "IAyHbFP85bf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define dataset directory in Google Drive\n",
        "data_dir = \"/content/drive/MyDrive/original\"  # Adjust this to your dataset path\n",
        "batch_size = 32\n",
        "num_classes = 6\n",
        "\n",
        "# Step 3: Define transformations for the dataset\n",
        "basic_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Step 4: Load dataset and split into train/validation sets (80%/20%)\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=basic_transforms)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Step 5: Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Step 6: Load EfficientNet B2 and modify the classifier\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.efficientnet_b2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 7: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 8: Define training function\n",
        "def train(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "# Step 9: Define validation function\n",
        "def validate(model, loader, criterion, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    tp = [0] * num_classes\n",
        "    fp = [0] * num_classes\n",
        "    fn = [0] * num_classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Calculate TP, FP, FN for each class\n",
        "            for i in range(num_classes):\n",
        "                tp[i] += torch.sum((preds == i) & (labels == i)).item()\n",
        "                fp[i] += torch.sum((preds == i) & (labels != i)).item()\n",
        "                fn[i] += torch.sum((preds != i) & (labels == i)).item()\n",
        "\n",
        "    accuracy = correct.double() / len(loader.dataset)\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        print(f\"Class {i} - TP: {tp[i]}, FP: {fp[i]}, FN: {fn[i]}\")\n",
        "\n",
        "    return avg_loss, accuracy, tp, fp, fn\n",
        "\n",
        "# Step 10: Training loop\n",
        "num_epochs = 7\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy, tp, fp, fn = validate(model, val_loader, criterion, num_classes)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Step 11: Save the trained model to Google Drive\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/banana_disease_efficientnet_b2.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqKXWpw4TEbu",
        "outputId": "f30b3ef1-05a1-4a30-cb80-96c1c02e4a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Class 0 - TP: 11, FP: 4, FN: 2\n",
            "Class 1 - TP: 32, FP: 5, FN: 0\n",
            "Class 2 - TP: 8, FP: 3, FN: 5\n",
            "Class 3 - TP: 17, FP: 0, FN: 0\n",
            "Class 4 - TP: 5, FP: 0, FN: 0\n",
            "Class 5 - TP: 1, FP: 0, FN: 5\n",
            "Epoch 1/7\n",
            "Train Loss: 0.9450, Validation Loss: 0.4394, Validation Accuracy: 0.8605\n",
            "Class 0 - TP: 12, FP: 0, FN: 1\n",
            "Class 1 - TP: 32, FP: 1, FN: 0\n",
            "Class 2 - TP: 12, FP: 0, FN: 1\n",
            "Class 3 - TP: 12, FP: 0, FN: 5\n",
            "Class 4 - TP: 5, FP: 5, FN: 0\n",
            "Class 5 - TP: 6, FP: 1, FN: 0\n",
            "Epoch 2/7\n",
            "Train Loss: 0.2304, Validation Loss: 0.1905, Validation Accuracy: 0.9186\n",
            "Class 0 - TP: 11, FP: 0, FN: 2\n",
            "Class 1 - TP: 32, FP: 0, FN: 0\n",
            "Class 2 - TP: 13, FP: 0, FN: 0\n",
            "Class 3 - TP: 17, FP: 0, FN: 0\n",
            "Class 4 - TP: 5, FP: 1, FN: 0\n",
            "Class 5 - TP: 6, FP: 1, FN: 0\n",
            "Epoch 3/7\n",
            "Train Loss: 0.1943, Validation Loss: 0.0876, Validation Accuracy: 0.9767\n",
            "Class 0 - TP: 10, FP: 0, FN: 3\n",
            "Class 1 - TP: 32, FP: 1, FN: 0\n",
            "Class 2 - TP: 11, FP: 0, FN: 2\n",
            "Class 3 - TP: 17, FP: 0, FN: 0\n",
            "Class 4 - TP: 5, FP: 1, FN: 0\n",
            "Class 5 - TP: 6, FP: 3, FN: 0\n",
            "Epoch 4/7\n",
            "Train Loss: 0.1208, Validation Loss: 0.2449, Validation Accuracy: 0.9419\n",
            "Class 0 - TP: 12, FP: 3, FN: 1\n",
            "Class 1 - TP: 31, FP: 3, FN: 1\n",
            "Class 2 - TP: 10, FP: 1, FN: 3\n",
            "Class 3 - TP: 17, FP: 0, FN: 0\n",
            "Class 4 - TP: 5, FP: 1, FN: 0\n",
            "Class 5 - TP: 3, FP: 0, FN: 3\n",
            "Epoch 5/7\n",
            "Train Loss: 0.0979, Validation Loss: 0.3696, Validation Accuracy: 0.9070\n",
            "Class 0 - TP: 10, FP: 2, FN: 3\n",
            "Class 1 - TP: 32, FP: 1, FN: 0\n",
            "Class 2 - TP: 11, FP: 0, FN: 2\n",
            "Class 3 - TP: 16, FP: 0, FN: 1\n",
            "Class 4 - TP: 5, FP: 1, FN: 0\n",
            "Class 5 - TP: 6, FP: 2, FN: 0\n",
            "Epoch 6/7\n",
            "Train Loss: 0.1936, Validation Loss: 0.2122, Validation Accuracy: 0.9302\n",
            "Class 0 - TP: 9, FP: 3, FN: 4\n",
            "Class 1 - TP: 31, FP: 0, FN: 1\n",
            "Class 2 - TP: 13, FP: 4, FN: 0\n",
            "Class 3 - TP: 16, FP: 0, FN: 1\n",
            "Class 4 - TP: 5, FP: 1, FN: 0\n",
            "Class 5 - TP: 3, FP: 1, FN: 3\n",
            "Epoch 7/7\n",
            "Train Loss: 0.2504, Validation Loss: 0.4764, Validation Accuracy: 0.8953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z93OIgj-3VFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}